{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dHqQyxdiMpkp","executionInfo":{"status":"ok","timestamp":1714159334493,"user_tz":-120,"elapsed":22784,"user":{"displayName":"Luca Wiehe","userId":"05054678043137946684"}},"outputId":"3868a106-d1dd-485e-d31a-55b3f7e62964"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","['.git', '.gitignore', '.ipynb_checkpoints', 'PeRFception-ScanNet', 'README.md', 'data', 'data_loader.ipynb', 'untitled']\n"]}],"source":["from google.colab import drive\n","import os\n","\n","# set path to project folder\n","gdrive_path='/content/gdrive/MyDrive/1-university/masters/2-semester/in2390_adl4cv/nerf_segmentation/' # Luca's Path\n","# gdrive_path='/content/gdrive/MyDrive/...' # Luis' Path\n","\n","# mount Google Drive\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","# navigate to Google Drive folder\n","os.chdir(gdrive_path)\n","\n","# check that we are in the right folder\n","print(sorted(os.listdir()))"]},{"cell_type":"markdown","source":["Execute the following code from your machine's terminal:\n","```\n","git clone https://huggingface.co/datasets/YWjimmy/PeRFception-ScanNet\n","```\n","\n","Then compress the repository into a .zip-file and place it inside the `/data/` folder of this repository. This step is necessary to allow the execution of this notebook inside Google Colab. In our next step, we will unzip the file inside of this Colab session:\n"],"metadata":{"id":"WAGLCZ7P8RzS"}},{"cell_type":"code","source":["!unzip \"/content/gdrive/MyDrive/1-university/masters/2-semester/in2390_adl4cv/nerf_segmentation/data/PeRFception-ScanNet.zip\""],"metadata":{"id":"kHaG9PssQ3A7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714159409955,"user_tz":-120,"elapsed":64471,"user":{"displayName":"Luca Wiehe","userId":"05054678043137946684"}},"outputId":"cfed5bb5-b910-4e39-c49f-762f108b2a23"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /content/gdrive/MyDrive/1-university/masters/2-semester/in2390_adl4cv/nerf_segmentation/data/PeRFception-ScanNet.zip\n","replace PeRFception-ScanNet/README.md? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n","replace PeRFception-ScanNet/.gitattributes? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n","replace PeRFception-ScanNet/plenoxel_scannet_scene0067_00/thick.npy? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n","replace PeRFception-ScanNet/plenoxel_scannet_scene0067_00/last.ckpt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "]}]},{"cell_type":"code","source":["!python -m pip install torch torchvision torchaudio plyfile MinkowskiEngine"],"metadata":{"id":"HAEoIcqf3Eqy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"3ppI_AhQWoa3","executionInfo":{"status":"ok","timestamp":1714167663036,"user_tz":-120,"elapsed":243,"user":{"displayName":"Luca Wiehe","userId":"05054678043137946684"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["MinkowskiEngine is not compatible with Python3.10. You have to go to `/usr/local/lib/python3.10/dist-packages/MinkowskiEngine/` and adjust some packages to use updated imports for `Sequence` that was previously part of `collections` and is now part of `collections.abc` as mentioned here: https://github.com/NVIDIA/MinkowskiEngine/issues/526"],"metadata":{"id":"Qqv3CuU3OB_W"}},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","import numpy as np\n","import logging\n","import MinkowskiEngine as ME\n","from plyfile import PlyData\n","\n","def load_ply(filename, load_label=False, load_instance=False):\n","    plydata = PlyData.read(filename)\n","    data = plydata.elements[0].data\n","    coords = np.array([data[\"x\"], data[\"y\"], data[\"z\"]], dtype=np.float32).T\n","    feats = np.array([data[\"red\"], data[\"green\"], data[\"blue\"]], dtype=np.float32).T\n","    return_args = [coords, feats]\n","    if load_label:\n","        labels = np.array(data[\"label\"], dtype=np.int32)\n","        return_args.append(labels)\n","    if load_instance:\n","        instances = np.array(data[\"instance\"], dtype=np.int32)\n","    else:\n","        instances = np.ones(coords.shape[0], dtype=np.int32)\n","    return_args.append(instances)\n","    return tuple(return_args)\n","\n","class PlenoxelScannetDataset(Dataset):\n","    DATA_PATH_FILE = {\n","        \"train\": \"scannetv2_train.txt\",\n","        \"val\": \"scannetv2_val.txt\",\n","        \"test\": \"scannetv2_test.txt\",\n","    }\n","\n","    def __init__(\n","        self,\n","        phase: str,\n","        data_root: str = \"datasets/scannet\",\n","        downsample_voxel_size=None,  # in meter\n","        voxel_size=0.02,\n","        train_transformations=[\n","            \"ChromaticTranslation\",\n","            \"ChromaticJitter\",\n","            \"CoordinateDropout\",\n","            \"RandomHorizontalFlip\",\n","            \"RandomAffine\",\n","            \"RandomTranslation\",\n","            \"NormalizeColor\",\n","        ],\n","        eval_transformations=[\n","            \"NormalizeColor\",\n","        ],\n","        ignore_label=-100,\n","        features=[\"colors\"],  # [\"colors\", \"xyzs\"]\n","    ):\n","        Dataset.__init__(self)\n","        self.ignore_label = ignore_label\n","        self.data_root = data_root\n","        self.phase = phase\n","        transformations = (\n","            train_transformations if phase == \"train\" else eval_transformations\n","        )\n","\n","        #self.transformations = transforms.Compose(\n","        #    [transforms.__dict__[t]() for t in transformations]\n","        #)\n","\n","        self.pc_files = []\n","        with open(os.path.join(self.data_root, self.DATA_PATH_FILE[phase]), \"r\") as f:\n","            self.pc_files.extend([l.rstrip(\"\\n\") for l in f.readlines()])\n","\n","        if downsample_voxel_size is None:\n","            downsample_voxel_size = voxel_size / 2\n","        self.downsample_voxel_size = downsample_voxel_size\n","        self.voxel_size = voxel_size\n","\n","        # map labels not evaluated to ignore_label\n","        label_map, n_used = dict(), 0\n","        for l in range(self.NUM_LABELS):\n","            if l in self.IGNORE_LABELS:\n","                label_map[l] = ignore_label\n","            else:\n","                label_map[l] = n_used\n","                n_used += 1\n","        label_map[ignore_label] = ignore_label\n","        self.label_map = label_map\n","        self.features = features\n","\n","    def __len__(self):\n","        return len(self.pc_files)\n","\n","    def __getitem__(self, i: int):\n","        xyzs, colors, labels, instances = load_ply(\n","            os.path.join(self.data_root, self.pc_files[i]),\n","            load_label=True,\n","            load_instance=True,\n","        )\n","\n","        if self.downsample_voxel_size > 0:\n","            pre_len = len(xyzs)\n","            _, colors, labels, row_inds = ME.utils.sparse_quantize(\n","                np.ascontiguousarray(xyzs),\n","                colors,\n","                labels=labels,\n","                quantization_size=self.downsample_voxel_size,\n","                return_index=True,\n","                ignore_label=self.ignore_label,\n","            )\n","            # Maintain the continuous coordinates\n","            xyzs = xyzs[row_inds] / self.voxel_size\n","            instances = instances[row_inds]\n","            logging.debug(f\"Downsampled point cloud index {i} from {pre_len} to {xyzs}\")\n","        else:\n","            xyzs /= self.voxel_size\n","\n","        xyzs, colors, labels = self.transformations(xyzs, colors, labels)\n","        # instances_info = self.get_instance_info(xyzs, instances)\n","        # centers = instances_info[\"center\"]\n","        # instance_ids = instances_info[\"ids\"]\n","        if self.IGNORE_LABELS is not None:\n","            labels = np.array([self.label_map[x] for x in labels], dtype=np.int)\n","\n","        features = []\n","        for f in self.features:\n","            features.append(eval(f))\n","        return {\n","            \"coordinates\": xyzs.astype(np.float32),\n","            \"features\": np.concatenate(features, axis=1).astype(np.float32),\n","            \"labels\": labels,\n","            \"colors\": colors,\n","            # \"instance_centers\": centers,\n","            # \"instance_ids\": instance_ids,\n","            \"dataset\": \"scannet\",\n","        }\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HzDwpeiZnqhm","executionInfo":{"status":"ok","timestamp":1714167250371,"user_tz":-120,"elapsed":2,"user":{"displayName":"Luca Wiehe","userId":"05054678043137946684"}},"outputId":"9d0448b0-4c85-433c-ce0b-05b6683f3800"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/MinkowskiEngine/__init__.py:221: UserWarning: The MinkowskiEngine was compiled with CPU_ONLY flag. If you want to compile with CUDA support, make sure `torch.cuda.is_available()` is True when you install MinkowskiEngine.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["dataset = PlenoxelScannetDataset(\"train\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":287},"id":"dfc6YYMrLwh7","executionInfo":{"status":"error","timestamp":1714167938687,"user_tz":-120,"elapsed":240,"user":{"displayName":"Luca Wiehe","userId":"05054678043137946684"}},"outputId":"6a9f0143-b475-4f7c-9231-3b639b7daae9"},"execution_count":19,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'datasets/scannet/scannetv2_train.txt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-919a7ab585a3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlenoxelScannetDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-17-91a9409d273d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, phase, data_root, downsample_voxel_size, voxel_size, train_transformations, eval_transformations, ignore_label, features)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpc_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA_PATH_FILE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpc_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/scannet/scannetv2_train.txt'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"pN0UWMKVXvtZ"},"execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}