{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMmSpfM2HvJG7R+7ki9OKsz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Luca-Wiehe/nerf_segmentation/blob/main/visual_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79ZCF1_JZGfn",
        "outputId": "42c42824-0136-42ea-c319-f9c6e6af5710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "['.git', 'README.md', 'Untitled', 'data', 'data_loader.ipynb', 'example_arrows', 'gitignore', 'open_nerf.ipynb', 'outputs']\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# set path to project folder\n",
        "# gdrive_path='/content/gdrive/MyDrive/1-university/masters/2-semester/in2390_adl4cv/nerf_segmentation/' # Luca's Path\n",
        "gdrive_path='/content/gdrive/MyDrive/Uni/adl4vc/nerf_segmentation/' # Luis' Path\n",
        "\n",
        "# mount Google Drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "# navigate to Google Drive folder\n",
        "os.chdir(gdrive_path)\n",
        "\n",
        "# check that we are in the right folder\n",
        "print(sorted(os.listdir()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building Dependencies. Takes about 20 min and you need to be connected to a GPU"
      ],
      "metadata": {
        "id": "zSEqU4t9dy_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install --upgrade pip\n",
        "!python -m pip install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia\n",
        "!python -m pip install ninja git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch"
      ],
      "metadata": {
        "id": "gbDKZnvAb04E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install -e /content/gdrive/MyDrive/Uni/adl4vc/opennerf"
      ],
      "metadata": {
        "id": "Yb4eO1RRkBwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install open3d\n",
        "!python -m pip install pyviz3d"
      ],
      "metadata": {
        "id": "OYip5Ao1V2Gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install nerfstudio\n",
        "!ns-install-cli"
      ],
      "metadata": {
        "id": "TU5HRPFbklvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methods from OpenNerf Repo"
      ],
      "metadata": {
        "id": "v3wYH20r5YRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from typing import Optional, List\n",
        "import numpy as np\n",
        "import json\n",
        "import open3d as o3d\n",
        "\n",
        "#import replica\n",
        "\n",
        "# import openreno.utils as utils\n",
        "\n",
        "from nerfstudio.process_data import process_data_utils, record3d_utils\n",
        "from nerfstudio.process_data.process_data_utils import CAMERA_MODELS\n",
        "\n",
        "def process_txt(filename):\n",
        "    with open(filename) as file:\n",
        "        lines = file.readlines()\n",
        "        lines = [line.rstrip() for line in lines]\n",
        "    return lines\n",
        "\n",
        "\n",
        "def process_replica(data: Path, output_dir: Path):\n",
        "    \"\"\"Process Replica data into a nerfstudio dataset.\n",
        "\n",
        "    This script does the following:\n",
        "\n",
        "    1. Scales images to a specified size.\n",
        "    2. Converts Record3D poses into the nerfstudio format.\n",
        "    \"\"\"\n",
        "\n",
        "    mesh_path = data / '..' / 'office0_mesh.ply'  # why do we need this?\n",
        "    scene_point_cloud = o3d.io.read_point_cloud(str(mesh_path))\n",
        "    verbose = True\n",
        "    num_downscales = 3\n",
        "    \"\"\"Number of times to downscale the images. Downscales by 2 each time. For example a value of 3\n",
        "        will downscale the images by 2x, 4x, and 8x.\"\"\"\n",
        "    max_dataset_size = 200\n",
        "    \"\"\"Max number of images to train on. If the dataset has more, images will be sampled approximately evenly. If -1,\n",
        "    use all images.\"\"\"\n",
        "\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    image_dir = output_dir / \"images\"\n",
        "    image_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    summary_log = []\n",
        "\n",
        "    replica_image_dir = data / \"results\"\n",
        "\n",
        "    if not replica_image_dir.exists():\n",
        "        raise ValueError(f\"Image directory {replica_image_dir} doesn't exist\")\n",
        "\n",
        "    replica_image_filenames = []\n",
        "    for f in replica_image_dir.iterdir():\n",
        "        if f.stem.startswith('frame'):  # removes possible duplicate images (for example, 123(3).jpg)\n",
        "            if f.suffix.lower() in [\".jpg\"]:\n",
        "                replica_image_filenames.append(f)\n",
        "\n",
        "    replica_image_filenames = sorted(replica_image_filenames)\n",
        "    num_images = len(replica_image_filenames)\n",
        "    idx = np.arange(num_images)\n",
        "    if max_dataset_size != -1 and num_images > max_dataset_size:\n",
        "        idx = np.round(np.linspace(0, num_images - 1, max_dataset_size)).astype(int)\n",
        "\n",
        "    replica_image_filenames = list(np.array(replica_image_filenames)[idx])\n",
        "\n",
        "    # Copy images to output directory\n",
        "    copied_image_paths = process_data_utils.copy_images_list(\n",
        "        replica_image_filenames,\n",
        "        image_dir=image_dir,\n",
        "        verbose=verbose,\n",
        "        num_downscales=num_downscales,\n",
        "    )\n",
        "    num_frames = len(copied_image_paths)\n",
        "\n",
        "    copied_image_paths = [Path(\"images/\" + copied_image_path.name) for copied_image_path in copied_image_paths]\n",
        "    summary_log.append(f\"Used {num_frames} images out of {num_images} total\")\n",
        "    if max_dataset_size > 0:\n",
        "        summary_log.append(\n",
        "            \"To change the size of the dataset add the argument [yellow]--max_dataset_size[/yellow] to \"\n",
        "            f\"larger than the current value ({max_dataset_size}), or -1 to use all images.\"\n",
        "        )\n",
        "\n",
        "    traj_path = data / \"traj.txt\"\n",
        "    replica_to_json(copied_image_paths, traj_path, output_dir, indices=idx, scene_point_cloud=scene_point_cloud)\n",
        "\n",
        "\n",
        "def replica_to_json(images_paths: List[Path], trajectory_txt: Path, output_dir: Path, indices: np.ndarray, scene_point_cloud) -> int:\n",
        "    \"\"\"Converts Replica's metadata and image paths to a JSON file.\n",
        "\n",
        "    Args:\n",
        "        images_paths: list if image paths.\n",
        "        traj_path: Path to the Replica trajectory file.\n",
        "        output_dir: Path to the output directory.\n",
        "        indices: Indices to sample the metadata_path. Should be the same length as images_paths.\n",
        "\n",
        "    Returns:\n",
        "        The number of registered images.\n",
        "    \"\"\"\n",
        "\n",
        "    assert len(images_paths) == len(indices)\n",
        "\n",
        "    # metadata_dict = io.load_from_json(metadata_path)\n",
        "    # poses_data = np.array(metadata_dict[\"poses\"])  # (N, 3, 4)\n",
        "\n",
        "    poses_data = process_txt(trajectory_txt)\n",
        "    poses_data = np.array(\n",
        "            [np.array(\n",
        "                [float(v) for v in p.split()]).reshape((4, 4)) for p in poses_data]\n",
        "        )\n",
        "    # NB: Record3D / scipy use \"scalar-last\" format quaternions (x y z w)\n",
        "    # https://fzheng.me/2017/11/12/quaternion_conventions_en/\n",
        "    # camera_to_worlds = np.concatenate(\n",
        "    #     [Rotation.from_quat(poses_data[:, :4]).as_matrix(), poses_data[:, 4:, None]],\n",
        "    #     axis=-1,\n",
        "    # ).astype(np.float32)\n",
        "\n",
        "    rot_x = np.eye(4)\n",
        "    a = np.pi\n",
        "    rot_x[1, 1] = np.cos(a)\n",
        "    rot_x[2, 2] = np.cos(a)\n",
        "    rot_x[1, 2] = -np.sin(a)\n",
        "    rot_x[2, 1] = np.sin(a)\n",
        "\n",
        "    camera_to_worlds = poses_data[indices] @ rot_x\n",
        "\n",
        "    import pyviz3d.visualizer as viz\n",
        "    v = viz.Visualizer()\n",
        "    for i in range(camera_to_worlds.shape[0]):\n",
        "        c2w = camera_to_worlds[i, 0:3, :]\n",
        "        origin = c2w @ np.array([0, 0, 0, 1])\n",
        "        v.add_arrow(f'{i};Arrow_1', start=origin, end=c2w @ np.array([0.1, 0.0, 0.0, 1]), color=np.array([255, 0, 0]), stroke_width=0.005, head_width=0.01)\n",
        "        v.add_arrow(f'{i};Arrow_2', start=origin, end=c2w @ np.array([0.0, 0.1, 0.0, 1]), color=np.array([0, 255, 0]), stroke_width=0.005, head_width=0.01)\n",
        "        v.add_arrow(f'{i};Arrow_3', start=origin, end=c2w @ np.array([0.0, 0.0, 0.1, 1]), color=np.array([0, 0, 255]), stroke_width=0.005, head_width=0.01)\n",
        "    v.add_points('scene', np.array(scene_point_cloud.points), np.array(scene_point_cloud.colors) * 255, np.array(scene_point_cloud.normals))\n",
        "    v.save('example_arrows')\n",
        "\n",
        "    frames = []\n",
        "    for i, im_path in enumerate(images_paths):\n",
        "        c2w = camera_to_worlds[i]\n",
        "        frame = {\n",
        "            \"file_path\": im_path.as_posix(),\n",
        "            \"transform_matrix\": c2w.tolist(),\n",
        "        }\n",
        "        frames.append(frame)\n",
        "\n",
        "    with open(trajectory_txt.parents[1] / 'cam_params.json') as file:\n",
        "        cam_params = json.load(file)\n",
        "\n",
        "    # Camera intrinsics\n",
        "    # K = np.array(metadata_dict[\"K\"]).reshape((3, 3)).T\n",
        "    focal_length = cam_params['camera']['fx']  # K[0, 0]\n",
        "\n",
        "    H = cam_params['camera']['h']\n",
        "    W = cam_params['camera']['w']\n",
        "\n",
        "    # TODO(akristoffersen): The metadata dict comes with principle points,\n",
        "    # but caused errors in image coord indexing. Should update once that is fixed.\n",
        "    cx, cy = W / 2.0, H / 2.0\n",
        "\n",
        "    out = {\n",
        "        \"fl_x\": focal_length,\n",
        "        \"fl_y\": focal_length,\n",
        "        \"cx\": cx,\n",
        "        \"cy\": cy,\n",
        "        \"w\": W,\n",
        "        \"h\": H,\n",
        "        \"camera_model\": CAMERA_MODELS[\"perspective\"].name,\n",
        "    }\n",
        "\n",
        "    out[\"frames\"] = frames\n",
        "\n",
        "    with open(output_dir / \"transforms.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(out, f, indent=4)\n",
        "    return len(frames)\n",
        "\n",
        "\n",
        "def visualize_lerf_trajector(dir):\n",
        "\n",
        "    with open(dir / 'transforms.json') as f:\n",
        "        j = json.load(f)\n",
        "\n",
        "    import pyviz3d.visualizer as viz\n",
        "    v = viz.Visualizer()\n",
        "\n",
        "    for i, frame in enumerate(j['frames'][::1]):\n",
        "        c2w = np.array(frame['transform_matrix']).reshape(4,4)[0:3, :]\n",
        "        origin = c2w @ np.array([0, 0, 0, 1])\n",
        "        v.add_arrow(f'{i};Arrow_1', start=origin, end=c2w @ np.array([0.1, 0.0, 0.0, 1]), color=np.array([255, 0, 0]), stroke_width=0.005, head_width=0.01)\n",
        "        v.add_arrow(f'{i};Arrow_2', start=origin, end=c2w @ np.array([0.0, 0.1, 0.0, 1]), color=np.array([0, 255, 0]), stroke_width=0.005, head_width=0.01)\n",
        "        v.add_arrow(f'{i};Arrow_3', start=origin, end=c2w @ np.array([0.0, 0.0, 0.1, 1]), color=np.array([0, 0, 255]), stroke_width=0.005, head_width=0.01)\n",
        "    v.save(dir / 'visualization')"
      ],
      "metadata": {
        "id": "Zg9l1MFnVexY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing Replica Dataset\n",
        "\n",
        "Download Replica dataset from wget https://cvg-data.inf.ethz.ch/nice-slam/data/Replica.zip\n",
        "\n"
      ],
      "metadata": {
        "id": "k1lYxF3y47vE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scenes = ['office0'] #, 'office1', 'office2', 'office3', 'office4', 'room0', 'room1', 'room2',]"
      ],
      "metadata": {
        "id": "wCMj2t2E5UgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for scene in scenes:\n",
        "    data = f'/content/gdrive/MyDrive/Uni/adl4vc/nerf_segmentation/data/replica_raw/{scene}'\n",
        "    output_dir = f'/content/gdrive/MyDrive/Uni/adl4vc/nerf_segmentation/data/nerfstudio_/replica_{scene}'\n",
        "    process_replica(Path(data), Path(output_dir))\n",
        "    visualize_lerf_trajector(Path(output_dir))"
      ],
      "metadata": {
        "id": "hUqavIUAXCaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(6008)\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "iEiBzo-nokYi",
        "outputId": "34b259ce-dc09-4f80-da25-e0e342895718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://wixx435t3yh-496ff2e9c6d22116-6008-colab.googleusercontent.com/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/gdrive/MyDrive/Uni/adl4vc/nerf_segmentation/data/nerfstudio_/replica_office0/visualization && python -m http.server 6008"
      ],
      "metadata": {
        "id": "8xJZRBO0qCfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing LERF Dataset\n",
        "\n",
        "The following command will generate a lerf visualization. This is saved in the stated directory. I havent figured out how to access a \"local google colab server\" from the colab notebook but to visualize it you can just download the \"visualisation\" subdir and execute the commands in you local terminal"
      ],
      "metadata": {
        "id": "Hy43EljhqMv4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the LERF Dataset from: https://drive.google.com/drive/folders/1vh0mSl7v29yaGsxleadcj-LCZOE_WEWB"
      ],
      "metadata": {
        "id": "W0Z4ld2AuNVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lerf_dir = '/content/gdrive/MyDrive/Uni/adl4vc/nerf_segmentation/data/lerf/bouquet'\n",
        "visualize_lerf_trajector(Path(lerf_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSbJI18vnWKL",
        "outputId": "ce312299-88fc-4669-bcd4-7dd1f73b8a47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "************************************************************************\n",
            "1) Start local server:\n",
            "    cd /content/gdrive/MyDrive/Uni/adl4vc/nerf_segmentation/data/lerf/bouquet/visualization; python -m http.server 6008\n",
            "2) Open in browser:\n",
            "    http://localhost:6008\n",
            "************************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/gdrive/MyDrive/Uni/adl4vc/nerf_segmentation/data/lerf/bouquet && python -m http.server 6008"
      ],
      "metadata": {
        "id": "LqYPJz4-HuNo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}